{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pathlib import Path\n",
    "import sklearn.model_selection\n",
    "import tensorflow as tf\n",
    "from neuralspot.tflite.metrics import MultiF1Score\n",
    "from sleepkit.defines import SKTrainParams, get_sleep_stage_classes, get_sleep_stage_class_names, get_sleep_stage_class_mapping\n",
    "from sleepkit.datasets import Hdf5Dataset\n",
    "from sleepkit.utils import env_flag, set_random_seed, setup_logger\n",
    "from sleepkit.metrics import compute_iou, confusion_matrix_plot\n",
    "from sleepkit.datasets.utils import create_dataset_from_data\n",
    "\n",
    "from sleepkit.defines import SKTrainParams\n",
    "from neuralspot.tflite.metrics import get_flops\n",
    "from neuralspot.tflite.model import get_strategy\n",
    "from sleepkit.models.unet import UNet, UNetParams, UNetBlockParams\n",
    "from sleepkit.models.UNext import UNext, UNextParams, UNextBlockParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SKTrainParams(\n",
    "    job_dir=Path(\"..\", \"results\", \"mesa-fs001\", \"experiment-002\"),\n",
    "    ds_path=Path(\"..\", \"datasets\", \"processed\", \"mesa-fs001\"),\n",
    "    sampling_rate=2,\n",
    "    frame_size=120,\n",
    "    samples_per_subject=200,\n",
    "    val_samples_per_subject=200,\n",
    "    val_subjects=0.2,\n",
    "    batch_size=256,\n",
    "    buffer_size=50000,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=100,\n",
    "    val_metric=\"loss\",\n",
    "    val_size=50000,\n",
    "    # Extra params\n",
    "    lr_rate=5e-3,\n",
    "    lr_cycles=1,\n",
    "    label_smoothing=0.1,\n",
    "    num_sleep_stages=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_v1(inputs: tf.Tensor, num_classes: int = 2):\n",
    "    blocks = [\n",
    "        UNetBlockParams(filters=12, depth=1, kernel=(1, 5), strides=(1, 2), skip=True, seperable=True, dropout=0.2),\n",
    "        UNetBlockParams(filters=24, depth=1, kernel=(1, 5), strides=(1, 2), skip=True, seperable=True, dropout=0.2),\n",
    "        UNetBlockParams(filters=36, depth=1, kernel=(1, 5), strides=(1, 2), skip=True, seperable=False, dropout=0.2),\n",
    "    ]\n",
    "    return UNet(\n",
    "        inputs,\n",
    "        params=UNetParams(\n",
    "            blocks=blocks,\n",
    "            output_kernel_size=(1, 5),\n",
    "            include_top=True,\n",
    "            use_logits=False,\n",
    "            include_rnn=False,\n",
    "        ),\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "def load_model(inputs: tf.Tensor, num_classes: int = 2):\n",
    "    y = inputs   \n",
    "    y = tf.keras.layers.Conv1D(filters=24, kernel_size=9, strides=1, padding=\"same\")(y)\n",
    "    # y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Activation(tf.nn.relu6)(y)\n",
    "    y = tf.keras.layers.SpatialDropout1D(rate=0.2)(y)\n",
    "\n",
    "    y = tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"same\")(y)\n",
    "    # y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Activation(tf.nn.relu6)(y)   \n",
    "    # y = tf.keras.layers.SpatialDropout1D(rate=0.2)(y) \n",
    "\n",
    "    # y = tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"same\")(y)\n",
    "    # y = tf.keras.layers.BatchNormalization()(y)\n",
    "    # y = tf.keras.layers.Activation(tf.nn.relu6)(y)    \n",
    "\n",
    "    y = tf.keras.layers.GRU(units=32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(y)\n",
    "    y = tf.keras.layers.GRU(units=24, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(y)\n",
    "\n",
    "    y = tf.keras.layers.RepeatVector(inputs.shape[1])(y)\n",
    "    y = tf.keras.layers.GRU(units=24, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(y)\n",
    "    y = tf.keras.layers.GRU(units=16, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(y)\n",
    "    y = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes))(y)\n",
    "    y = tf.keras.layers.Softmax()(y)\n",
    "    model = tf.keras.Model(inputs, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(x, y, num_classes, class_map: dict[int, int]):\n",
    "    return (\n",
    "        x,\n",
    "        #tf.one_hot(class_map.get(sts.mode(y[-5:]).mode, 0), num_classes)\n",
    "        tf.one_hot(np.vectorize(class_map.get)(y), num_classes)\n",
    "    )\n",
    "\n",
    "def load_train_datasets(params: SKTrainParams, feat_shape, class_shape, class_map, feat_cols=None):\n",
    "    def preprocess(x: npt.NDArray[np.float32]):\n",
    "       return x + np.random.normal(0, 0.05, size=x.shape)\n",
    "\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=feat_shape, dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=class_shape, dtype=tf.int32),\n",
    "    ) \n",
    " \n",
    "    ds = Hdf5Dataset(\n",
    "        ds_path=params.ds_path,\n",
    "        frame_size=params.frame_size,\n",
    "        mask_key=\"mask\",\n",
    "        feat_cols=feat_cols,\n",
    "    )\n",
    "    train_subject_ids, val_subject_ids = sklearn.model_selection.train_test_split(\n",
    "        ds.train_subject_ids, test_size=params.val_subjects\n",
    "    )\n",
    "\n",
    "    def train_generator(subject_ids):\n",
    "        def ds_gen():\n",
    "            train_subj_gen = ds.uniform_subject_generator(subject_ids)\n",
    "            return map(\n",
    "                lambda x_y: prepare(preprocess(x_y[0]), x_y[1], class_shape[-1], class_map),\n",
    "                ds.signal_generator(train_subj_gen, samples_per_subject=params.samples_per_subject)\n",
    "            )\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            ds_gen,\n",
    "            output_signature=output_signature,\n",
    "        )\n",
    "\n",
    "    split = len(train_subject_ids) // params.data_parallelism\n",
    "    train_datasets = [train_generator(\n",
    "        train_subject_ids[i * split : (i + 1) * split]\n",
    "    ) for i in range(params.data_parallelism)]\n",
    "\n",
    "    # Create TF datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        train_datasets\n",
    "    ).interleave(\n",
    "        lambda x: x,\n",
    "        cycle_length=params.data_parallelism,\n",
    "        deterministic=False,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    ).shuffle(\n",
    "        buffer_size=params.buffer_size,\n",
    "        reshuffle_each_iteration=True,\n",
    "    ).batch(\n",
    "        batch_size=params.batch_size,\n",
    "        drop_remainder=False,\n",
    "    ).prefetch(\n",
    "        buffer_size=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    def val_generator():\n",
    "        val_subj_gen = ds.uniform_subject_generator(val_subject_ids)\n",
    "        return map(\n",
    "            lambda x_y: prepare(preprocess(x_y[0]), x_y[1], class_shape[-1], class_map),\n",
    "            ds.signal_generator(val_subj_gen, samples_per_subject=params.samples_per_subject)\n",
    "        )\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_generator(\n",
    "        generator=val_generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    val_x, val_y = next(val_ds.batch(params.val_size).as_numpy_iterator())\n",
    "    val_ds = create_dataset_from_data(\n",
    "        val_x, val_y, output_signature=output_signature\n",
    "    ).batch(\n",
    "        batch_size=params.batch_size,\n",
    "        drop_remainder=False,\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.seed = set_random_seed(params.seed)\n",
    "logger.info(f\"Random seed {params.seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sleep_stages = 2\n",
    "\n",
    "feat_names = [\n",
    "    \"SPO2-mu\",  # 0\n",
    "    \"SPO2-std\", # 1\n",
    "    \"SPO2-med\", # 2\n",
    "    \"SPO2-iqr\", # 3\n",
    "    \"MOV-mu\",   # 4\n",
    "    \"MOV-std\",  # 5 \n",
    "    \"MOV-med\",  # 6\n",
    "    \"MOV-iqr\",  # 7\n",
    "    \"RRI-mu\",   # 8\n",
    "    \"RRI-std\",  # 9\n",
    "    \"RRI-med\",  # 10\n",
    "    \"RRI-iqr\",  # 11\n",
    "    \"RRI-sd-rms\", # 12\n",
    "    \"RRI-sd-std\", # 13\n",
    "    \"HR-bpm\",     # 14\n",
    "    \"RSP-bpm\",    # 15\n",
    "    \"HRV-lf\",     # 16\n",
    "    \"HRV-hf\",     # 17\n",
    "    \"HRV-lfhf\"    # 18\n",
    "]\n",
    "feat_cols = list(range(len(feat_names)))\n",
    "# feat_cols = [\n",
    "#     4, 5, 6, 7,\n",
    "#     8, 9, 10, 11, 12, 14\n",
    "# ]\n",
    "\n",
    "num_feats = len(feat_cols)\n",
    "target_classes = get_sleep_stage_classes(num_sleep_stages)\n",
    "class_names = get_sleep_stage_class_names(num_sleep_stages)\n",
    "class_mapping = get_sleep_stage_class_mapping(num_sleep_stages)\n",
    "num_classes = len(target_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(params.job_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_shape = (params.frame_size, num_feats)\n",
    "class_shape = (params.frame_size, num_classes)\n",
    "inputs = tf.keras.Input(feat_shape, batch_size=None, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = get_strategy()\n",
    "with strategy.scope():\n",
    "    print(\"Loading datasets...\")\n",
    "    train_ds, val_ds = load_train_datasets(params, feat_shape, class_shape, class_mapping, feat_cols=feat_cols)\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model(inputs, num_classes=len(target_classes))\n",
    "    flops = get_flops(model, batch_size=1)\n",
    "\n",
    "    # Grab optional LR parameters\n",
    "    lr_rate: float = getattr(params, \"lr_rate\", 1e-3)\n",
    "    lr_cycles: int = getattr(params, \"lr_cycles\", 1)\n",
    "    steps_per_epoch = params.steps_per_epoch or 1000\n",
    "    if lr_cycles == 1:\n",
    "        scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate=lr_rate,\n",
    "            decay_steps=int(steps_per_epoch * params.epochs),\n",
    "        )\n",
    "    else:\n",
    "        scheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            initial_learning_rate=lr_rate,\n",
    "            first_decay_steps=int(0.1 * steps_per_epoch * params.epochs),\n",
    "            t_mul=1.65 / (0.1 * lr_cycles * (lr_cycles - 1)),\n",
    "            m_mul=0.4,\n",
    "        )\n",
    "    optimizer = tf.keras.optimizers.Adam(scheduler)\n",
    "    loss = tf.keras.losses.CategoricalFocalCrossentropy(\n",
    "        from_logits=False,\n",
    "        label_smoothing=getattr(params, \"label_smoothing\", 0.1),\n",
    "    )\n",
    "    metrics = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "        MultiF1Score(name=\"f1\", dtype=tf.float32, average=\"macro\"),\n",
    "        tf.keras.metrics.OneHotIoU(\n",
    "            num_classes=len(target_classes),\n",
    "            target_class_ids=target_classes,\n",
    "            name=\"iou\",\n",
    "        ),\n",
    "    ]    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    model(inputs)\n",
    "\n",
    "    model.summary(print_fn=logger.info)\n",
    "    logger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\n",
    "\n",
    "    params.weights_file = str(params.job_dir / \"model.weights\")\n",
    "\n",
    "    model_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=f\"val_{params.val_metric}\",\n",
    "            patience=max(int(0.25 * params.epochs), 1),\n",
    "            mode=\"max\" if params.val_metric == \"f1\" else \"auto\",\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=params.weights_file,\n",
    "            monitor=f\"val_{params.val_metric}\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"max\" if params.val_metric == \"f1\" else \"auto\",\n",
    "            verbose=1,\n",
    "        ),\n",
    "        tf.keras.callbacks.CSVLogger(str(params.job_dir / \"history.csv\")),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=str(params.job_dir / \"logs\"),\n",
    "            write_steps_per_second=True\n",
    "        ),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_ds,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            verbose=2,\n",
    "            epochs=params.epochs,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=model_callbacks,\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Stopping training due to keyboard interrupt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(params.weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [y.numpy() for _, y in val_ds]\n",
    "y_true = np.argmax(np.concatenate(test_labels).squeeze(), axis=-1)\n",
    "y_pred = np.argmax(model.predict(val_ds).squeeze(), axis=-1)\n",
    "\n",
    "# Summarize results\n",
    "test_acc = np.sum(y_pred == y_true) / y_true.size\n",
    "test_iou = compute_iou(y_true, y_pred, average=\"weighted\")\n",
    "f1_metric = MultiF1Score(name=\"f1\", dtype=tf.float32, average=\"weighted\")\n",
    "f1_metric.update_state(y_true=y_true, y_pred=y_pred)\n",
    "test_f1 = f1_metric.result().numpy()\n",
    "# test_f1 = 0\n",
    "logger.info(f\"[TEST SET] ACC={test_acc:.2%}, IoU={test_iou:.2%} F1={test_f1:.2%}\")\n",
    "\n",
    "cm_path = str(params.job_dir / f\"confusion_matrix_test{num_sleep_stages}_unet.png\")\n",
    "confusion_matrix_plot(\n",
    "    y_true.flatten(),\n",
    "    y_pred.flatten(),\n",
    "    labels=class_names,\n",
    "    save_path=cm_path,\n",
    "    normalize=\"true\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
